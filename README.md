# SyncTalk 2D

SyncTalk_2D is a 2D lip-sync video generation model based on SyncTalk and [Ultralight-Digital-Human](https://github.com/anliyuan/Ultralight-Digital-Human). It can generate lip-sync videos with high quality and low latency, and it can also be used for real-time lip-sync video generation.

Compared to the Ultralight-Digital-Human, we have improved the audio feature encoder and increased the resolution to 328 to accommodate higher-resolution input video. This version can realize high-definition, commercial-grade digital humans.

ä¸Ultralight-Digital-Humanç›¸æ¯”ï¼Œæˆ‘ä»¬æ”¹è¿›äº†éŸ³é¢‘ç‰¹å¾ç¼–ç å™¨ï¼Œå¹¶å°†åˆ†è¾¨ç‡æå‡è‡³328ä»¥é€‚åº”æ›´é«˜åˆ†è¾¨ç‡çš„è¾“å…¥è§†é¢‘ã€‚è¯¥ç‰ˆæœ¬å¯å®ç°é«˜æ¸…ã€å•†ä¸šçº§æ•°å­—äººã€‚

## Features

- ğŸ“¹ **High Resolution**: Supports 328x328 resolution (vs 256x256 in original)
- ğŸµ **Advanced Audio Processing**: Enhanced audio feature encoder with AVE support
- ğŸš€ **Low Latency**: Optimized for real-time generation
- ğŸŒ **Web Interface**: User-friendly Gradio interface for easy interaction
- ğŸ”„ **Seamless Looping**: Videos can start and end with the same frame

## Setting up

Set up the environment:

### Windows
```bash
conda create -n synctalk_2d python=3.10
conda activate synctalk_2d
```

### Linux
```bash
conda create -n synctalk_2d python=3.10
conda activate synctalk_2d
```

Install dependencies:

### Windows
```bash
# Install PyTorch with CUDA support
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# Install FFmpeg (very important)
conda install -c conda-forge ffmpeg=4.3.* x264

# Install other dependencies
pip install -r requirements.txt
```

### Linux
```bash
# Install PyTorch with CUDA support
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# Install FFmpeg (very important)
conda install -c conda-forge 'ffmpeg=4.3.*' x264

# Install other dependencies
pip install -r requirements.txt
```

## Quick Start - Web Interface

Launch the Gradio interface:
```bash
python app_gradio.py
```

Then open http://localhost:7860 in your browser.

## Prepare your data

1. Record a 5-minute video with your head facing the camera and without significant movement. At the same time, ensure that the camera does not move and the background light remains unchanged during video recording.
2. Don't worry about FPS, the code will automatically convert the video to 25fps.
3. No second person's voice can appear in the recorded video, and a 5-second silent clip is left at the beginning and end of the video.
4. Don't wear clothes with overly obvious texture, it's better to wear single-color clothes.
5. The video should be recorded in a well-lit environment.
6. The audio should be clear and without background noise.

## Train

1. Put your video in the 'dataset/name/name.mp4' 
   - Example: dataset/May/May.mp4

2. Run the process and training script:

   ### Linux/macOS
   ```bash
   bash scripts/training_328.sh name gpu_id
   ```
   - Example: `bash scripts/training_328.sh May 0`
   
   ### Windows
   
   Option 1: Using the provided batch script (recommended)
   ```cmd
   scripts\training_328.bat name gpu_id
   ```
   - Example: `scripts\training_328.bat May 0`
   
   Option 2: Using Git Bash or WSL
   ```bash
   bash scripts/training_328.sh name gpu_id
   ```
   
   Option 3: Using Command Prompt/PowerShell (run commands separately)
   ```cmd
   set CUDA_VISIBLE_DEVICES=gpu_id
   python data_utils/process.py ./dataset/name/name.mp4
   python synctalk/core/syncnet_328.py --save_dir ./syncnet_ckpt/name --dataset_dir ./dataset/name --asr ave
   # Find the latest checkpoint file in ./syncnet_ckpt/name/*.pth
   python scripts/train_328.py --dataset_dir ./dataset/name --save_dir ./checkpoint/name --asr ave --use_syncnet --syncnet_checkpoint ./syncnet_ckpt/name/your_checkpoint.pth
   ```
   - Replace `name` with your dataset name and `gpu_id` with your GPU device ID (e.g., 0)
   - Replace `your_checkpoint.pth` with the actual checkpoint filename generated in the syncnet_ckpt folder
   
   **Note:** 
   - Training takes approximately 5 hours
   - If OOM (Out of Memory) occurs, try reducing the batch_size in the training script
   - Windows users: Ensure CUDA toolkit is properly installed and NVIDIA GPU drivers are up to date
   - Windows paths: Use forward slashes (/) or escaped backslashes (\\) in paths
   - **NEW**: Training now automatically resumes from interruptions using `.training_state.json`
   - **NEW**: SyncNet training is automatically skipped if already completed

## Inference

### Command Line
```bash
python scripts/inference_cli.py --name <model_name> --audio_path <audio.wav>
```

### Batch Processing
```bash
./batch_inference.sh <model_name> <audio_directory>
```

### Web Interface
Use the Gradio interface for an easier experience with model selection and real-time preview.

## Documentation

- [Architecture Overview](docs/ARCHITECTURE.md) - Detailed model architecture and workflow
- [Gradio Interface Guide](docs/README_gradio.md) - How to use the web interface

## Project Structure

```
SyncTalk_2D/
â”œâ”€â”€ app_gradio.py       # Web interface
â”œâ”€â”€ run_synctalk.py     # Unified inference interface
â”œâ”€â”€ synctalk/           # Core package
â”‚   â”œâ”€â”€ core/           # Model implementations
â”‚   â””â”€â”€ utils/          # Utilities
â”œâ”€â”€ scripts/            # Training and inference scripts
â”‚   â”œâ”€â”€ train_328.py    # Training script (328x328)
â”‚   â”œâ”€â”€ inference_cli.py # Command-line inference
â”‚   â””â”€â”€ preprocess_data.py # Data preprocessing
â”œâ”€â”€ data_utils/         # Data preprocessing tools
â”œâ”€â”€ checkpoint/         # Trained model checkpoints
â”œâ”€â”€ dataset/            # Training data
â”œâ”€â”€ demo/               # Demo audio files
â”œâ”€â”€ docs/               # Documentation
â””â”€â”€ gradio_outputs/     # Generated videos from web interface
```

## Available Models

Currently trained models in `checkpoint/`:
- **AD2.2** - Commercial avatar model
- **LS1** - Another trained avatar

## Tips

- For best results, use clear speech audio without background noise
- The model works best with the same language it was trained on
- Videos are generated at 25 FPS
- Use the "Loop back to start frame" option in Gradio for seamless loops

## Citation

If you use this code, please cite the original papers:
- SyncTalk paper
- Ultralight-Digital-Human repository
